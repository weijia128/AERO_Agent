# CLAUDE_DEV.md - Developer Documentation

This document contains detailed technical documentation for developers working on the Airport Emergency Response Agent system.

For high-level overview and quick start, see **CLAUDE.md**.

## Table of Contents

1. [Detailed Data Flows](#detailed-data-flows)
2. [Scenario Configuration](#scenario-configuration)
3. [Tool System Details](#tool-system-details)
4. [Risk Assessment Rules](#risk-assessment-rules)
5. [FSM System](#fsm-system)
6. [Constraint System](#constraint-system)
7. [Topology Analysis](#topology-analysis)
8. [Radiotelephony Normalization](#radiotelephony-normalization)
9. [LLM Configuration](#llm-configuration)
10. [Semantic Understanding Module](#semantic-understanding-module)
11. [Tool Development Guide](#tool-development-guide)
12. [Code Quality Guidelines](#code-quality-guidelines)
13. [Production Readiness Checklist](#production-readiness-checklist)

---

## Detailed Data Flows

### User Input â†’ Entity Extraction Flow

```python
# agent/nodes/input_parser.py

User Input (Chinese text)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. normalize_radiotelephony_text()                  â”‚
â”‚    åŸºç¡€è§„èŒƒåŒ–: æ´â†’0, å¹ºâ†’1, æ‹â†’7                      â”‚
â”‚    è·‘é“æ–¹å‘æ ‡è¯†: è·‘é“27å·¦â†’è·‘é“27L (ICAOæ ¼å¼)         â”‚
â”‚    ä» data/raw/Radiotelephony_ATC.json åŠ è½½è§„åˆ™     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. RadiotelephonyNormalizer (LLM + è§„åˆ™æ£€ç´¢)         â”‚
â”‚    â”œâ”€ retrieve_examples()                           â”‚
â”‚    â”‚  # å…³é”®è¯åŒ¹é…æ£€ç´¢ Few-shot ç¤ºä¾‹                 â”‚
â”‚    â”‚  # æå–: runway, taxiway, stand, flight        â”‚
â”‚    â”œâ”€ _build_prompt()                               â”‚
â”‚    â”‚  # æ„å»º Few-shot æç¤ºè¯                         â”‚
â”‚    â”œâ”€ LLM.invoke()                                  â”‚
â”‚    â”‚  # è¯­ä¹‰è§„èŒƒåŒ– (5ç§’è¶…æ—¶)                         â”‚
â”‚    â””â”€ è¿”å› {                                        â”‚
â”‚          normalized_text,                           â”‚
â”‚          entities {flight_no, position, ...},       â”‚
â”‚          confidence                                 â”‚
â”‚       }                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
identify_scenario()
    # Match keywords against ScenarioRegistry
    # Returns: "oil_spill", "bird_strike", etc.
    â†“
Entity extraction (depends on ENABLE_SEMANTIC_UNDERSTANDING)
    â”œâ”€ If enabled:
    â”‚  â”œâ”€ understand_conversation() â†’ LLM + history extraction
    â”‚  â”œâ”€ split_by_confidence() â†’ accepted vs low-confidence
    â”‚  â””â”€ deterministic extract_entities() â†’ regexè¡¥å……
    â””â”€ If disabled:
       â”œâ”€ extract_entities_hybrid() â†’ regex + LLM
       â””â”€ Merge: Normalizer entities > Hybrid extraction
    â†“
apply_auto_enrichment()  # ğŸ”„ Parallel execution
    â”œâ”€ Phase 1: Independent queries (ThreadPoolExecutor, max 3 workers)
    â”‚  â”œâ”€ get_aircraft_info(flight_no) â†’ aircraft details
    â”‚  â”œâ”€ flight_plan_lookup(flight_no) â†’ flight schedule
    â”‚  â””â”€ get_stand_location(position) â†’ stand coordinates + topology
    â”‚
    â”œâ”€ Phase 2: Dependent calculations (requires Phase 1 results)
    â”‚  â”œâ”€ calculate_impact_zone(position, fluid_type, risk_level)
    â”‚  â”‚  # BFS graph diffusion algorithm
    â”‚  â”‚  # Rules: FUEL HIGH=3 hops, MEDIUM=2 hops, etc.
    â”‚  â””â”€ analyze_position_impact(position)
    â”‚     # Direct impact analysis + adjacent facilities
    â”‚
    â””â”€ Timeout handling: 10s per future, graceful degradation
    â†“
update_checklist()
    # Mark collected fields as complete in state.checklist
    â†“
Output: Updated AgentState
    â”œâ”€ incident: enriched with auto-fetched data
    â”œâ”€ checklist: {fluid_type: true, position: true, ...}
    â”œâ”€ spatial_analysis: {affected_taxiways, affected_runways, ...}
    â”œâ”€ flight_plan_table: flight schedule data
    â””â”€ observations: enrichment process records
```

### Automatic Weather Query Flow

```
Position known â†’ reasoning_node auto trigger â†’ tool_executor(get_weather)
    â†“
get_weather(location=incident.position)
    â”œâ”€ Normalize location (e.g., è·‘é“27L â†’ 27L)
    â”œâ”€ Query latest record from data/processed/awos_weather_*.csv
    â””â”€ If missing: fallback to nearest observation point with warning
```

- Weather is queried once per position (repeat only if position changes)
- If input text indicates a runway, `position_display` keeps the "è·‘é“" prefix for UI/report output

### Risk Assessment â†’ Spatial Analysis Flow

```python
# tools/assessment/assess_risk.py

Current incident (fluid_type, engine_status, continuous, leak_size)
    â†“
assess_risk_tool.execute()
    â”œâ”€ Load rules from scenario or defaults
    â”œâ”€ Match against RISK_RULES (priority-ordered, 12 rules):
    â”‚  1. (FUEL + continuous + RUNNING) â†’ HIGH (95 pts)
    â”‚  2. (FUEL + RUNNING) â†’ HIGH (90 pts)
    â”‚  3. (FUEL + continuous) â†’ HIGH (85 pts)
    â”‚  ...
    â”‚  12. (OIL) â†’ LOW (25 pts)
    â”‚
    â”œâ”€ Return: {level, score, factors, immediate_actions}
    â”‚  # level: "HIGH", "MEDIUM_HIGH", "MEDIUM", "LOW"
    â”‚  # score: 0-100 numerical score
    â”‚  # factors: ["èˆªç©ºç‡ƒæ²¹", "å‘åŠ¨æœºè¿è½¬", ...]
    â”‚  # immediate_actions: ["å…³é—­å‘åŠ¨æœº", "æ³¡æ²«è¦†ç›–", ...]
    â”‚
    â””â”€ Update state.risk_assessment
    â†“
calculate_impact_zone_tool.execute()
    â”œâ”€ Load airport topology (NetworkX graph from JSON)
    â”‚  # Nodes: stands, taxiways, runways with lat/lon
    â”‚  # Edges: connectivity between nodes
    â”‚
    â”œâ”€ Find start node (nearest to position)
    â”‚
    â”œâ”€ Look up spread rule from SPREAD_RULES
    â”‚  FUEL:
    â”‚    HIGH: radius=3, runway_impact=true
    â”‚    MEDIUM: radius=2, runway_impact=true
    â”‚    LOW: radius=1, runway_impact=false
    â”‚  HYDRAULIC: radius=2/1/1, no runway
    â”‚  OIL: radius=1/1/0
    â”‚
    â”œâ”€ BFS spread from start node (breadth-first search)
    â”‚  # Explore graph up to radius hops
    â”‚
    â”œâ”€ Classify nodes: taxiway | runway | stand
    â”‚
    â”œâ”€ Check runway adjacency (if rule.runway_impact)
    â”‚
    â””â”€ Return: {isolated_nodes, affected_taxiways, affected_runways}
    â†“
analyze_position_impact_tool.execute()
    â”œâ”€ Analyze direct impact on facility
    â”œâ”€ Estimate closure time (based on fluid type + risk level)
    â”œâ”€ Calculate severity score (1-10)
    â””â”€ Identify adjacent affected facilities
    â†“
predict_flight_impact_tool.execute() [âš ï¸ Partially implemented]
    â”œâ”€ Query flight plan database
    â”œâ”€ Match flights to affected stands/runways/taxiways
    â”œâ”€ Calculate delay predictions
    â””â”€ Generate severity distribution
```

### FSM Validation â†’ Mandatory Actions Flow

```python
# agent/nodes/fsm_validator.py

AgentState (after critical tool execution: assess_risk, calculate_impact_zone, notify_department)
    â†“
fsm_validator_node()
    â”œâ”€ Get validator: FSMValidator(FSMEngine)
    â”‚
    â”œâ”€ Call validate(agent_state):
    â”‚  â”œâ”€ sync_with_agent_state()
    â”‚  â”‚  # Infer current FSM state from Agent completion
    â”‚  â”‚  # Example: checklist.p1_complete=true â†’ P1_RISK_ASSESS
    â”‚  â”‚            mandatory.risk_assessed=true â†’ P2_IMMEDIATE_CONTROL
    â”‚  â”‚
    â”‚  â”œâ”€ check_preconditions(target_state, agent_state)
    â”‚  â”‚  # For each precondition (e.g., "checklist.fluid_type"):
    â”‚  â”‚  #   Check if satisfied; add error if not
    â”‚  â”‚  # Example: Entering P2 requires mandatory.risk_assessed=true
    â”‚  â”‚
    â”‚  â””â”€ check_mandatory_actions(agent_state)
    â”‚     â”œâ”€ For each MandatoryAction:
    â”‚     â”‚  â”œâ”€ Evaluate condition (e.g., risk_level == "HIGH")
    â”‚     â”‚  â”œâ”€ If triggered: check if completed
    â”‚     â”‚  â””â”€ If not completed: add to pending_actions
    â”‚     â”‚
    â”‚     # Example:
    â”‚     # Condition: risk_level == "HIGH"
    â”‚     # Action: notify_department(department: æ¶ˆé˜², priority: immediate)
    â”‚     # Check field: fire_dept_notified
    â”‚     #
    â”‚     â””â”€ Return (errors, pending_actions)
    â”‚
    â””â”€ Return: FSMValidationResult
       â”œâ”€ is_valid: boolean
       â”œâ”€ current_state: FSM state before validation
       â”œâ”€ inferred_state: FSM state after validation
       â”œâ”€ errors: ["è¿›å…¥P2éœ€è¦å…ˆå®Œæˆrisk_assessed", ...]
       â””â”€ pending_actions: [{action: "notify_department", params: {...}}, ...]
    â†“
Routing decision (after_fsm_validation):
    â”œâ”€ If errors: â†’ reasoning (Agent needs to fix)
    â”‚  # FSM error messages guide Agent remediation
    â”‚  # Example: "é«˜å±æƒ…å†µå¿…é¡»é€šçŸ¥æ¶ˆé˜²" â†’ Agent calls notify_department
    â”‚
    â”œâ”€ If COMPLETED state: â†’ output_generator
    â”‚
    â”œâ”€ If P8_CLOSE + pending_actions: â†’ reasoning (trigger forced actions)
    â”‚
    â””â”€ Otherwise: â†’ reasoning (continue)
```

### Report Generation â†’ Final Output Flow

```python
# agent/nodes/output_generator.py

Complete AgentState
    â†“
output_generator_node()
    â”œâ”€ Build affected areas text (from spatial_analysis)
    â”‚  # Format: "501æœºä½ã€æ»‘è¡Œé“A1/A2ã€è·‘é“09"
    â”‚
    â”œâ”€ Build event context
    â”‚  â”œâ”€ incident: position, fluid_type, engine_status, continuous
    â”‚  â”œâ”€ risk_assessment: level, score, factors
    â”‚  â””â”€ spatial_analysis: impact zone, affected facilities
    â”‚
    â”œâ”€ Collect handling process (from actions_taken)
    â”‚  # Timeline of tool executions with timestamps
    â”‚  # Example: [
    â”‚  #   "14:30 - é£é™©è¯„ä¼°:HIGHçº§é£é™©(90åˆ†)",
    â”‚  #   "14:32 - é€šçŸ¥æ¶ˆé˜²éƒ¨é—¨:å·²åˆ°è¾¾ç°åœº",
    â”‚  #   ...
    â”‚  # ]
    â”‚
    â”œâ”€ Collect notifications sent
    â”‚  # List of notified departments with priority
    â”‚
    â”œâ”€ Prepare recommendations (based on risk level)
    â”‚  # HIGH: "ç«‹å³å…³é—­å‘åŠ¨æœº", "æ³¡æ²«è¦†ç›–", ...
    â”‚  # MEDIUM: "æ¸…æ±¡äººå‘˜å°±ä½", "é˜²æ»‘å¤„ç†", ...
    â”‚
    â”œâ”€ Call LLM to generate summary (optional)
    â”‚  # Narrative summary of incident handling
    â”‚
    â”œâ”€ Render final report from template [âš ï¸ Currently: 778-line string concatenation]
    â”‚  # Report structure:
    â”‚  #   - Title, event summary, risk level
    â”‚  #   - Handling process (timeline of actions)
    â”‚  #   - Checklist items (P1/P2 fields)
    â”‚  #   - Coordination units notified
    â”‚  #   - Operational impact (affected flights, closure time)
    â”‚  #   - Recommendations (safety measures, follow-up)
    â”‚  #   - Timestamp
    â”‚
    â””â”€ Return: final_report (dict) + final_answer (str)
    â†“
tool_executor_node() [if generate_report action]
    â”œâ”€ Detect report_generated flag
    â”œâ”€ Call output_generator_node()
    â”œâ”€ Set awaiting_user = True
    â”œâ”€ Set next_node = "end"
    â””â”€ Wait for user confirmation
```

### Bird Strike Risk Assessment (BSRC)

```python
# tools/assessment/assess_bird_strike_risk.py

incident (phase, affected_part, event_type, current_status, crew_request, ...)
    â†“
assess_bird_strike_risk.execute()
    â”œâ”€ Normalize inputs: phase/impact_area/evidence/bird_info/ops_impact
    â”œâ”€ Weighted score + rule boosts (BSRC.json)
    â”œâ”€ Apply risk floor overrides (R1-R4)
    â””â”€ Update state.risk_assessment + mandatory_actions_done.risk_assessed
```

---

## Scenario Configuration

### Dynamic Prompt Loading

Each scenario has `prompt.yaml` defining its system prompt and field collection order:

```yaml
# scenarios/oil_spill/prompt.yaml

system_prompt: |
  ä½ æ˜¯æœºåœºæœºåªåº”æ€¥å“åº”ä¸“å®¶ Agent...

field_order:           # ä¿¡æ¯æ”¶é›†é¡ºåº(å¼ºåˆ¶æŒ‰åºè¯¢é—®)
  - flight_no
  - position
  - fluid_type
  - engine_status
  - continuous

field_names:           # å­—æ®µä¸­æ–‡åç§°æ˜ å°„
  flight_no: èˆªç­å·
  position: äº‹å‘ä½ç½®
  fluid_type: æ¶²ä½“ç±»å‹

ask_prompts:           # å„å­—æ®µçš„è¿½é—®æç¤º
  flight_no: "è¯·æä¾›æ¶‰äº‹é£æœºçš„èˆªç­å·?"
  position: "è¯·æŠ¥å‘Šäº‹ä»¶å‘ç”Ÿçš„å…·ä½“ä½ç½®?"
  fluid_type: "è¯·æè¿°æ³„æ¼æ¶²ä½“çš„ç±»å‹?"
```

### Checklist Hierarchy

**P1 fields** (must collect before risk assessment):
- Oil spill: `fluid_type`, `continuous`, `engine_status`, `position`
- Bird strike: `flight_no`, `position`, `event_type`, `affected_part`, `current_status`, `crew_request`

**P2 fields** (optional, enhances assessment accuracy):
- Oil spill: `leak_size`
- Bird strike: `tail_no`, `phase`, `evidence`, `bird_info`, `ops_impact`

See `scenarios/bird_strike/checklist.yaml` and `docs/SCENARIO_FIELD_CONTRACTS.md`.

---

## Tool System Details

### Tool Registry

Tools are registered with scenario tags:

```python
# tools/registry.py

ToolRegistry.register(
    AssessRiskTool(),
    scenarios=["oil_spill", "common"]
)
```

`ToolRegistry.get_by_scenario("oil_spill")` returns all tools tagged with `oil_spill` or `common`.

### Tool Categories

**Information Tools (6)**:
- `ask_for_detail`: Ask user for specific missing field with context-aware prompts
- `get_aircraft_info`: Retrieve flight information from database (auto-called when flight number detected)
- `flight_plan_lookup`: Query flight schedule from `data/raw/èˆªç­è®¡åˆ’/`
- `get_weather`: Query AWOS weather data from CSV/XLSX files
- `smart_ask`: Intelligently ask multiple related questions in one interaction
- `radiotelephony_normalizer`: ATC phonetic normalization (two-stage approach)

**Spatial Tools (5)**:
- `get_stand_location`: Find stand coordinates and adjacent facilities
- `calculate_impact_zone`: Graph-based BFS diffusion (auto-called when position detected)
- `analyze_position_impact`: Detailed impact analysis with closure time, severity score (1-10)
- `predict_flight_impact`: Flight impact prediction (âš ï¸ partially implemented)
- `topology_loader`: Load and manage airport topology graph (NetworkX)

**Knowledge Tools (1)**:
- `search_regulations`: RAG-style retrieval from emergency procedures knowledge base

**Assessment Tools (3)**:
- `assess_risk`: Compatibility shim for scenario-specific assessors
- `assess_oil_spill_risk`: 12-rule deterministic engine for FUEL/HYDRAULIC/OIL
- `assess_bird_strike_risk`: BSRC weighted scoring based on phase, evidence, bird characteristics

**Action Tools (2)**:
- `notify_department`: Send notifications to fire, ATC, maintenance, operations, etc.
- `generate_report`: Create final incident report with timeline and recommendations

### Knowledge Base

**Mock knowledge base** (`tools/knowledge/search_regulations.py`):
- Emergency procedures for fuel, hydraulic, and engine oil spills
- Each regulation includes: risk level, risk features, cleanup method, source
- Report generator references retrieved knowledge when generating reports

---

## Risk Assessment Rules

### Architecture Note

`tools/assessment/assess_risk.py` is a **compatibility shim** that imports scenario-specific assessment tools:
- `assess_oil_spill_risk.py` - For oil/fuel/hydraulic spills
- `assess_bird_strike_risk.py` - For bird strike incidents using BSRC rules

### Fluid Type Risk Matrix

**Oil Spill Risk Matrix** (`tools/assessment/assess_oil_spill_risk.py`):

| Fluid Type | Risk Level | Key Features | Cleanup Method |
|------------|------------|--------------|----------------|
| Aviation Fuel (FUEL) | HIGH | Flammable/explosive, foam coverage required | Absorbent materials + explosion-proof pump |
| Hydraulic Oil | MEDIUM-HIGH | Flammable, high-pressure jet hazard | Pressure relief first, then absorbent |
| Engine Oil (OIL) | MEDIUM | Combustible, toxic smoke | Absorbent materials + industrial cleaner |

### Immediate Actions by Risk Level

- **HIGH**: Notify fire department, shut down engine, evacuate, establish safety zone, foam coverage
- **MEDIUM-HIGH**: Fire department on standby, pressure relief, set up warning zone
- **MEDIUM**: Standby resources, absorbent materials, anti-slip treatment
- **LOW**: Maintenance notification, monitoring

---

## FSM System

### FSM Module Structure

```python
fsm/
â”œâ”€â”€ engine.py       # FSMEngine - Core state management logic
â”‚   â”œâ”€â”€ State transition rules
â”‚   â”œâ”€â”€ Precondition checking
â”‚   â””â”€â”€ State synchronization with AgentState
â”œâ”€â”€ validator.py    # FSMValidator - Validation interface
â”‚   â”œâ”€â”€ validate(agent_state) â†’ FSMValidationResult
â”‚   â”œâ”€â”€ check_preconditions()
â”‚   â””â”€â”€ check_mandatory_actions()
â”œâ”€â”€ states.py       # FSMState enum + transition definitions
â”‚   â””â”€â”€ INIT â†’ P1_RISK_ASSESS â†’ P2_IMMEDIATE_CONTROL â†’ ... â†’ COMPLETED
â””â”€â”€ transitions.py  # State transition matrix
```

### FSM State Flow

```
INIT                    # Initial state
  â†“
P1_RISK_ASSESS         # Risk assessment phase (collect P1 fields)
  â†“
P2_IMMEDIATE_CONTROL   # Immediate control actions
  â†“
P3_IMPACT_ANALYSIS     # Spatial impact analysis
  â†“
P4_NOTIFICATION        # Department notifications
  â†“
P5_MONITORING          # Situation monitoring
  â†“
P6_FOLLOWUP            # Follow-up actions
  â†“
P7_REPORTING           # Report generation
  â†“
P8_CLOSE               # Incident closure
  â†“
COMPLETED              # Final state
```

### Validation Triggers

FSM validation runs after critical tool executions:
- `assess_risk` â†’ validates risk assessment completion
- `calculate_impact_zone` â†’ validates spatial analysis
- `notify_department` â†’ validates notification requirements

### Validation Results

```python
FSMValidationResult:
  - is_valid: Boolean indicating compliance
  - current_state: FSM state before validation
  - inferred_state: FSM state after validation (may auto-advance)
  - errors: List of validation failures (e.g., "è¿›å…¥P2éœ€è¦å…ˆå®Œæˆrisk_assessed")
  - pending_actions: List of mandatory actions not yet completed
```

---

## Constraint System

### Constraint Module Structure

```python
constraints/
â”œâ”€â”€ checker.py   # ConstraintChecker - Rule evaluation engine
â”‚   â”œâ”€â”€ check_field_constraints()   # Validate field values
â”‚   â”œâ”€â”€ check_workflow_constraints() # Validate workflow rules
â”‚   â””â”€â”€ evaluate_condition()        # Dynamic rule evaluation
â””â”€â”€ loader.py    # ConstraintLoader - Load constraints from YAML
    â””â”€â”€ load_scenario_constraints()
```

### Mandatory Actions

Defined in `agent/state.py` + `fsm/`:
- `risk_assessed`: Must complete risk assessment before P2
- `fire_dept_notified`: Required for HIGH risk incidents
- `atc_notified`: Required for runway/taxiway impacts
- `impact_zone_calculated`: Required before notifications

### Constraint Evaluation

- Constraints loaded from `scenarios/<scenario>/config.yaml`
- Dynamic condition evaluation supports complex rules:
  ```python
  risk_level == "HIGH" AND position CONTAINS "runway"
  ```
- Violations block state transitions and trigger Agent remediation

---

## Topology Analysis

### Airport Topology Graph

**Data source** (`tools/spatial/topology_loader.py`):
- **Primary**: `scripts/data_processing/topology_clustering_based.json` (generated from trajectory clustering)
- **Alternate**: `data/spatial/airport_topology.json` (backup copy)

**Data structure**:
- Nodes: stands, taxiways, runways with lat/lon coordinates
- Edges: connectivity between nodes (undirected graph)
- NetworkX format for efficient graph algorithms

**Analysis methods**:
- BFS-based reachability analysis for impact zone calculation
- Graph diffusion with configurable radius (1-3 hops)
- Runway adjacency detection

### Automatic Analysis

When position is extracted in `input_parser.py`:
1. `get_stand_location` called automatically
2. Location details: coordinates, adjacent taxiways, nearest runway
3. Impact zone calculation: BFS diffusion based on fluid type and risk level
4. Results stored in `spatial_analysis` and `incident.impact_zone`

### Impact Zone Rules

| Fluid Type | Risk Level | BFS Radius | Runway Impact |
|------------|------------|------------|---------------|
| FUEL | HIGH | 3 hops | Yes |
| FUEL | MEDIUM | 2 hops | Yes |
| FUEL | LOW | 1 hop | No |
| HYDRAULIC | HIGH/MEDIUM | 2 hops | No |
| OIL | HIGH/MEDIUM | 1 hop | No |

---

## Radiotelephony Normalization

### Overview

Converts aviation radio telephony (ATC phonetic alphabet) to standard format using a two-stage approach.

### Implementation

**Stage 1: Basic rule-based normalization** (`agent/nodes/input_parser.py:135-175`):

```python
def normalize_radiotelephony_text(text: str) -> str:
    """
    åŸºç¡€è§„èŒƒåŒ–: æ•°å­—å’Œå­—æ¯è¯»æ³•è½¬æ¢
    - æ´â†’0, å¹ºâ†’1, ä¸¤â†’2, ä¸‰â†’3, æ‹â†’7, å…«â†’8, ä¹â†’9
    - é˜¿å°”æ³•â†’A, å¸ƒæ‹‰æ²ƒâ†’B, æŸ¥ç†â†’C
    - è§„èŒƒåŒ–ä½ç½®é¡ºåº: "12æ»‘è¡Œé“" â†’ "æ»‘è¡Œé“12"
    - è·‘é“æ–¹å‘æ ‡è¯†è½¬æ¢: "è·‘é“27å·¦" â†’ "è·‘é“27L" (ICAOæ ¼å¼)
      # é¿å…"è·‘é“27å·¦å‘ç”Ÿé¸Ÿå‡»"è¢«è¯¯è§£æä¸º"è·‘é“27"+"å·¦å‘"
    """
    # ä» Radiotelephony_ATC.json åŠ è½½è§„åˆ™
    digits_map = {"æ´": "0", "å¹º": "1", "æ‹": "7", ...}
    letters_map = {"é˜¿å°”æ³•": "A", "å¸ƒæ‹‰æ²ƒ": "B", ...}
    # ... æ‰§è¡Œæ›¿æ¢

    # è·‘é“æ–¹å‘æ ‡è¯†è½¬æ¢ (å·¦â†’L, å³â†’R, ä¸­â†’C)
    normalized = re.sub(
        r"(è·‘é“\d{1,2})(å·¦|å³|ä¸­)",
        lambda m: f"{m.group(1)}{'L' if m.group(2) == 'å·¦' else 'R' if m.group(2) == 'å³' else 'C'}",
        normalized,
    )
```

**Stage 2: LLM + Rule-based Few-shot retrieval** (`tools/information/radiotelephony_normalizer.py:31-238`):

```python
class RadiotelephonyNormalizer:
    """
    èˆªç©ºè¯»æ³•è§„èŒƒåŒ–å¼•æ“ (LLM + è§„åˆ™æ£€ç´¢,éå‘é‡ RAG)

    å·¥ä½œæµç¨‹:
    1. retrieve_examples(input_text)
       - å…³é”®è¯åŒ¹é…: æå– runway/taxiway/stand/flight/oil_spill/bird_strike
       - è§„åˆ™ç›¸ä¼¼åº¦: åŸºäºå…³é”®è¯é‡å åº¦æ‰“åˆ† (éå‘é‡ç›¸ä¼¼åº¦)
       - è¿”å› top-3 æœ€ç›¸ä¼¼ç¤ºä¾‹

    2. _build_prompt(text, examples)
       - åŠ è½½è½¬æ¢è§„åˆ™ä» Radiotelephony_ATC.json
       - æ„å»º Few-shot æç¤ºè¯

    3. normalize_with_llm(text, timeout=5)
       - è°ƒç”¨ LLM è¿›è¡Œè¯­ä¹‰è§„èŒƒåŒ–
       - è¿”å›æ ‡å‡†åŒ–å®ä½“å’Œç½®ä¿¡åº¦

    æ³¨æ„: å½“å‰å®ç°ä½¿ç”¨å…³é”®è¯åŒ¹é…,ä¸æ˜¯çœŸæ­£çš„å‘é‡ RAG
    """

    def retrieve_examples(self, input_text: str, top_k: int = 3):
        """æ£€ç´¢æœ€ç›¸ä¼¼çš„è§„èŒƒåŒ–ç¤ºä¾‹ (åŸºäºå…³é”®è¯,éå‘é‡)"""
        keywords = self._extract_keywords(input_text)
        # å…³é”®è¯: ["runway", "taxiway", "stand", "flight", "oil_spill", "bird_strike"]

        for example in examples:
            score = self._calculate_similarity(keywords, example["input"])
            # è§„åˆ™æ‰“åˆ†: å…³é”®è¯å‘½ä¸­ +1 åˆ†
        return top_k_examples
```

### Knowledge Base

**Radiotelephony_ATC.json**:

```json
{
  "digits": {
    "0": "æ´", "1": "å¹º", "2": "ä¸¤", "7": "æ‹", ...
  },
  "letters": {
    "A": "é˜¿å°”æ³•", "B": "å¸ƒæ‹‰æ²ƒ", "C": "æŸ¥ç†", ...
  },
  "normalization_rules": {
    "runway_formats": {
      "examples": [
        {"input": "è·‘é“æ´ä¸¤å·¦", "output": "02L"},
        {"input": "è·‘é“å¹ºå…«å³", "output": "18R"}
      ]
    },
    "flight_formats": {
      "airline_codes": {
        "å·èˆª": "3U", "å›½èˆª": "CA", "ä¸œèˆª": "MU", ...
      }
    }
  }
}
```

### Design Notes

**Current Implementation**: Rule-based keyword matching (not vector RAG)
- âœ… Fast, no external dependencies
- âœ… Sufficient for structured aviation data
- âš ï¸ Requires manual rule updates for new patterns

**Runway Direction Disambiguation** (è·‘é“æ–¹å‘æ ‡è¯†è½¬æ¢):
- Problem: "è·‘é“ä¸¤æ‹å·¦å‘ç”Ÿé¸Ÿå‡»" would be misparsed as position="è·‘é“27" + affected_part="å·¦å‘"
- Solution: In Stage 1, convert "è·‘é“XXå·¦/å³/ä¸­" to ICAO format "è·‘é“XXL/R/C"
- Effect: "L" in "è·‘é“27Lå‘ç”Ÿé¸Ÿå‡»" no longer conflicts with "å·¦å‘" regex

**Future Enhancement**: True vector-based RAG
- Requires: embedding model (e.g., sentence-transformers) + vector DB (Chroma/FAISS)
- Pros: Better semantic understanding, automatic pattern learning
- Cons: Additional dependencies, higher latency
- Decision: Defer until rule coverage proves insufficient

### Examples

| Input | Output | Entities |
|-------|--------|----------|
| å·èˆªä¸‰å¹ºæ‹æ‹ è·‘é“æ´ä¸¤å·¦ æŠ¥å‘Šé¸Ÿå‡» | å·èˆª3U3177 è·‘é“02L æŠ¥å‘Šé¸Ÿå‡» | {flight_no: "3U3177", position: "02L", event_type: "bird_strike"} |
| è·‘é“ä¸¤æ‹å·¦å‘ç”Ÿç¡®è®¤é¸Ÿå‡» | è·‘é“27Lå‘ç”Ÿç¡®è®¤é¸Ÿå‡» | {position: "è·‘é“27L", event_type: "ç¡®è®¤é¸Ÿå‡»"} |
| è·‘é“27Lå‘ç”Ÿé¸Ÿå‡» å·¦å‘å—æŸ | è·‘é“27Lå‘ç”Ÿé¸Ÿå‡» å·¦å‘å—æŸ | {position: "è·‘é“27L", affected_part: "å·¦å‘"} |
| äº”æ´å¹ºæœºä½å‘ç°ç‡ƒæ²¹æ³„æ¼ | 501æœºä½å‘ç°ç‡ƒæ²¹æ³„æ¼ | {position: "501", fluid_type: "FUEL"} |

---

## LLM Configuration

**config/llm_config.py**:
- `LLMClientFactory` supports zhipu (GLM-4) and OpenAI-compatible APIs
- Uses LangChain's `ChatOpenAI` or `ChatZhipuAI`

```python
from config.llm_config import LLMClientFactory

llm = LLMClientFactory.create(
    provider="zhipu",  # or "openai"
    model="glm-4",
    api_key="your_api_key"
)
```

---

## Semantic Understanding Module

### Overview

The semantic understanding module provides **optional LLM-driven entity extraction** with confidence scoring, complementing the default regex-based extraction.

### Configuration

```bash
# .env
ENABLE_SEMANTIC_UNDERSTANDING=true  # Default: false
```

### Implementation

**agent/nodes/semantic_understanding.py**:

When enabled, the input parser uses a **hybrid extraction strategy**:

1. **Semantic Extraction** (LLM-based):
   - `understand_conversation()` â†’ LLM analyzes user input + conversation history
   - Returns entities with confidence scores (0-1 scale)
   - Example output:
     ```python
     {
       "flight_no": {"value": "3U3177", "confidence": 0.95},
       "position": {"value": "501", "confidence": 0.90},
       "fluid_type": {"value": "FUEL", "confidence": 0.85}
     }
     ```

2. **Confidence Splitting**:
   - High confidence (â‰¥0.8): Entities accepted automatically
   - Low confidence (<0.8): Flagged for user clarification

3. **Regex Fallback**:
   - Deterministic `extract_entities()` regex patterns supplement LLM extraction
   - Ensures critical fields (position, flight_no) are never missed

### Workflow

```
Input â†’ RadiotelephonyNormalizer (always on)
      â†“
      If ENABLE_SEMANTIC_UNDERSTANDING:
        â†’ understand_conversation() â†’ LLM extraction
        â†’ split_by_confidence() â†’ High vs Low
        â†’ extract_entities() â†’ Regex supplement
      Else:
        â†’ extract_entities_hybrid() â†’ Regex + minimal LLM
```

### Benefits

- Better handling of ambiguous or colloquial input
- Context-aware extraction using conversation history
- Graceful degradation with confidence scoring

### Trade-offs

- Additional LLM call (adds ~1-2s latency)
- Slightly higher API costs
- May extract false positives with low confidence

### Recommendation

Enable for scenarios with complex natural language input; disable for structured/formulaic input to optimize latency.

---

## Tool Development Guide

### Creating a New Tool

**Step 1: Create tool file**

```python
# tools/category/my_tool.py

from typing import Dict, Any
from tools.base import BaseTool

class MyTool(BaseTool):
    """
    Brief description of what this tool does.

    This tool should be used when...
    """

    # Tool metadata
    name = "my_tool"
    description = "Clear description visible to LLM for tool selection"

    def execute(self, state: Dict[str, Any], inputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the tool logic.

        Args:
            state: Current AgentState dict
            inputs: Action inputs from LLM (action_input field)

        Returns:
            Dict with keys:
                - observation: String message shown to Agent
                - success: Boolean indicating success/failure
                - (optional) Additional state updates
        """
        # Extract inputs
        param1 = inputs.get("param1")
        param2 = inputs.get("param2", "default_value")

        # Validate inputs
        if not param1:
            return {
                "observation": "Error: param1 is required",
                "success": False
            }

        # Execute tool logic
        try:
            result = self._do_work(param1, param2)

            return {
                "observation": f"Successfully completed: {result}",
                "success": True,
                # Optional: Update state
                "state_updates": {
                    "my_data": result
                }
            }
        except Exception as e:
            return {
                "observation": f"Tool execution failed: {str(e)}",
                "success": False
            }

    def _do_work(self, param1: str, param2: str) -> Any:
        """Private helper method for actual work."""
        # Implementation here
        return "result"
```

**Step 2: Register tool in registry**

```python
# tools/registry.py

from tools.category.my_tool import MyTool

def register_all_tools():
    """Register all tools with the ToolRegistry."""

    # ... existing registrations ...

    # Register your tool
    ToolRegistry.register(
        MyTool(),
        scenarios=["oil_spill", "common"]  # Which scenarios can use this tool
    )
```

**Step 3: Add tests**

```python
# tests/tools/test_my_tool.py

import pytest
from tools.category.my_tool import MyTool

class TestMyTool:
    def test_execute_success(self):
        tool = MyTool()
        state = {"incident": {...}}
        inputs = {"param1": "value1"}

        result = tool.execute(state, inputs)

        assert result["success"] is True
        assert "Successfully completed" in result["observation"]

    def test_execute_missing_param(self):
        tool = MyTool()
        state = {}
        inputs = {}  # Missing param1

        result = tool.execute(state, inputs)

        assert result["success"] is False
        assert "param1 is required" in result["observation"]
```

### Tool Best Practices

1. **Clear naming**: Tool name should be action-oriented (`get_`, `calculate_`, `assess_`)
2. **Descriptive description**: LLM uses this to decide when to use the tool
3. **Input validation**: Always validate inputs before execution
4. **Error handling**: Return structured error messages in `observation`
5. **State updates**: Return `state_updates` dict to modify AgentState
6. **Deterministic when possible**: Avoid LLM calls in tools for calculable logic
7. **Idempotent**: Tools should be safe to call multiple times

### Example: Creating "Get Weather" Tool

```python
# tools/information/get_weather.py

from typing import Dict, Any
import requests
from tools.base import BaseTool

class GetWeatherTool(BaseTool):
    """
    Retrieves current weather conditions for a location.
    Use this tool when you need weather information to assess
    environmental impact on incident handling.
    """

    name = "get_weather"
    description = "Get current weather conditions (temperature, wind, precipitation) for a specific location"

    def execute(self, state: Dict[str, Any], inputs: Dict[str, Any]) -> Dict[str, Any]:
        location = inputs.get("location")

        if not location:
            return {
                "observation": "Error: Location parameter is required",
                "success": False
            }

        try:
            # Call weather API (example)
            weather_data = self._fetch_weather(location)

            observation = (
                f"Weather at {location}:\n"
                f"- Temperature: {weather_data['temp']}Â°C\n"
                f"- Wind: {weather_data['wind_speed']} m/s, {weather_data['wind_direction']}\n"
                f"- Conditions: {weather_data['conditions']}"
            )

            return {
                "observation": observation,
                "success": True,
                "state_updates": {
                    "weather": weather_data
                }
            }
        except Exception as e:
            return {
                "observation": f"Failed to fetch weather: {str(e)}",
                "success": False
            }

    def _fetch_weather(self, location: str) -> Dict[str, Any]:
        # Actual API call implementation
        return {
            "temp": 25,
            "wind_speed": 5,
            "wind_direction": "NE",
            "conditions": "Clear"
        }

# Register in tools/registry.py:
# ToolRegistry.register(GetWeatherTool(), scenarios=["oil_spill", "common"])
```

---

## Code Quality Guidelines

### Error Handling Patterns

**Good: Specific exception handling**
```python
try:
    result = tool.execute(state, inputs)
except ToolExecutionError as e:
    logger.error(f"Tool execution failed: {e}", exc_info=True)
    return {"observation": f"Error: {e}", "success": False}
except ValidationError as e:
    logger.warning(f"Invalid input: {e}")
    return {"observation": f"Invalid input: {e}", "success": False}
```

**Bad: Catching all exceptions silently**
```python
try:
    result = tool.execute(state, inputs)
except:  # Too broad, hides bugs
    pass  # Silent failure - never do this!
```

### Logging Best Practices

Add logging to critical paths:

```python
import logging
logger = logging.getLogger(__name__)

def input_parser_node(state: AgentState) -> AgentState:
    logger.info(f"Starting input parsing for session {state['session_id']}")

    entities = extract_entities_hybrid(message)
    logger.debug(f"Extracted entities: {entities}")

    if not entities.get("position"):
        logger.warning("Position not extracted from user input")

    return updated_state
```

**Log levels:**
- `DEBUG`: Detailed diagnostic information
- `INFO`: General informational messages (state transitions, tool executions)
- `WARNING`: Unexpected situations that don't block execution
- `ERROR`: Error events that may still allow continued operation
- `CRITICAL`: Severe errors causing system failure

### Type Annotation Requirements

All functions must have type hints:

```python
# Good
def calculate_risk(
    fluid_type: str,
    engine_status: str,
    continuous: bool
) -> RiskAssessment:
    ...

# Bad
def calculate_risk(fluid_type, engine_status, continuous):
    ...
```

Use TypedDict for complex dictionaries:

```python
from typing import TypedDict

class ToolResult(TypedDict):
    observation: str
    success: bool
    state_updates: Dict[str, Any]
```

### Testing Requirements

Every tool must have:
1. Success case test
2. Failure case test
3. Edge case tests

```python
@pytest.mark.parametrize("fluid_type,expected_level", [
    ("FUEL", "HIGH"),
    ("HYDRAULIC", "MEDIUM"),
    ("OIL", "LOW"),
])
def test_assess_risk_levels(fluid_type, expected_level):
    ...
```

---

## Production Readiness Checklist

### Must-Have (Blocking Production)

- [ ] **Persistent storage** (PostgreSQL/Redis) for sessions
  - Replace MemorySessionStore with database-backed storage
  - Implement session recovery after restart
  - Add session expiration and cleanup

- [ ] **Docker containerization** with docker-compose
  - Multi-stage Dockerfile for optimized image size
  - docker-compose.yml with all services (app, db, redis)
  - Environment variable management

- [ ] **Structured logging** (JSON format) in all critical paths
  - Replace print statements with logger calls
  - Add request ID tracing
  - Configure log aggregation (e.g., ELK stack)

- [ ] **Health check endpoint** (`/health` with liveness + readiness)
  - Liveness: Is the service running?
  - Readiness: Can the service handle requests?
  - Check dependencies (DB, LLM API)

- [ ] **Basic metrics** (request count, response time, active sessions)
  - Prometheus metrics endpoint
  - Custom business metrics (scenarios processed, tools used)
  - Grafana dashboards

- [ ] **Database for reports** (replace file-based storage)
  - PostgreSQL schema for reports
  - Report versioning and audit trail
  - Efficient querying and indexing

- [ ] **API authentication** (API key or JWT)
  - API key middleware
  - Rate limiting per key
  - Token rotation mechanism

- [ ] **Secrets management** (remove hardcoded API keys)
  - Use environment variables
  - Integrate with secrets manager (AWS Secrets Manager, Vault)
  - Rotate secrets regularly

### Should-Have (High Priority)

- [ ] **Configuration profiles** (dev/staging/prod separation)
  - Separate config files for each environment
  - Override mechanisms (env vars > config files)
  - Validation of required config

- [ ] **Comprehensive error handling** (custom exception hierarchy)
  - Define domain-specific exceptions
  - Consistent error response format
  - Client-friendly error messages

- [ ] **Input validation middleware** (centralized validation)
  - Pydantic models for all API requests
  - Automatic validation error responses
  - Sanitize user inputs

- [ ] **Rate limiting** (per-IP request throttling)
  - Token bucket or sliding window algorithm
  - Different limits for different endpoints
  - 429 responses with retry-after headers

- [ ] **CI/CD pipeline** (GitHub Actions for test + deploy)
  - Automated testing on PR
  - Linting and type checking
  - Automated deployment to staging/prod

- [ ] **Test coverage reporting** (pytest-cov with 80%+ target)
  - Coverage reports in CI
  - Enforce minimum coverage
  - Identify untested code paths

- [ ] **API documentation** (OpenAPI/Swagger specs)
  - Auto-generated from FastAPI
  - Interactive API explorer
  - Code examples in multiple languages

### Nice-to-Have (Enhancement)

- [ ] **Caching layer** (Redis for frequent queries)
  - Cache flight data, weather, topology
  - TTL-based invalidation
  - Cache warming on startup

- [ ] **Message queue** (Celery/RabbitMQ for async processing)
  - Offload long-running tasks
  - Report generation in background
  - Retry mechanism for failed tasks

- [ ] **Distributed tracing** (Jaeger/Datadog integration)
  - Trace requests across services
  - Identify performance bottlenecks
  - Root cause analysis for errors

- [ ] **Custom Prometheus metrics** (business-specific metrics)
  - Scenario completion rates
  - Average handling time per scenario
  - Tool usage statistics

- [ ] **Multi-language support** (i18n for prompts and reports)
  - Language detection from request
  - Translated prompts and responses
  - Locale-specific formatting

- [ ] **Automated rollback** (blue-green deployment)
  - Zero-downtime deployment
  - Health checks before traffic switch
  - Quick rollback on failure

### Current Status: 45% production-ready (Early Beta)

See [Production Readiness Assessment](./docs/PRODUCTION_READINESS.md) for detailed analysis.

### Performance Tuning

**Reduce auto-enrichment latency:**
- Increase ThreadPoolExecutor workers (default: 3)
- Cache flight data in Redis
- Pre-load topology graph on startup

**Optimize LLM calls:**
- Use shorter system prompts for simple scenarios
- Cache common LLM responses
- Use streaming for long outputs

**Database optimization:**
- Index session_id column
- Use connection pooling (SQLAlchemy)
- Implement read replicas for reporting queries

### Log Locations

- **Application logs**: stdout (capture with Docker logs)
- **LangSmith traces**: https://smith.langchain.com/
- **API request logs**: Check middleware logging in `apps/api/main.py`
- **Tool execution logs**: Currently minimal - add custom logging as needed

---

## Additional Resources

For more documentation, see:
- **CLAUDE.md**: High-level overview and quick start
- **docs/API_DOCUMENTATION.md**: API schemas and examples
- **docs/SCENARIO_FIELD_CONTRACTS.md**: Field definitions for each scenario
- **docs/ARCHITECTURE_DECISIONS.md**: Design decisions and trade-offs
- **docs/DEPLOYMENT_GUIDE.md**: Production deployment instructions
- **docs/PRODUCTION_READINESS.md**: Detailed production readiness assessment

## Testing Coverage

Test structure:
```
tests/
â”œâ”€â”€ agent/          # 8 node tests
â”œâ”€â”€ tools/          # Tool-specific unit tests
â”œâ”€â”€ fsm/            # FSM engine tests
â”œâ”€â”€ constraints/    # Constraint checker tests
â””â”€â”€ integration/    # End-to-end scenario tests
```

Run full test suite:
```bash
pytest tests/ -v
```

Run with coverage:
```bash
pytest tests/ --cov=agent --cov=tools --cov=fsm --cov-report=html
```
